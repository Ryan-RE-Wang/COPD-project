{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GradCAM import *\n",
    "from define_model import *\n",
    "from load_label import *\n",
    "from utilities import *\n",
    "from Parse_TFrecords import *\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "print(gpus)\n",
    " \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = 'emory'\n",
    "\n",
    "record_file_test = 'tfrecords/copd_{a}_test.tfrecords'.format(a=dataset)\n",
    "test_dataset = (tf.data.TFRecordDataset(\n",
    "    record_file_test, buffer_size=BATCH_SIZE, compression_type=None, num_parallel_reads=32)\n",
    ".map(parse_TFrecord_test))\n",
    "\n",
    "y_test, y_demo = get_data_label(dataset=dataset, split='test', return_demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'checkpoints_mimic/checkpoint_BCE_DenseNet121'\n",
    "archi = 'DenseNet121'\n",
    "model = define_model(archi)\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# For GradCam\n",
    "\n",
    "# last_conv_layer_name = 'relu'\n",
    "# classifier_layer_names = ['max_pool', 'dense']\n",
    "\n",
    "# # First, we create a model that maps the input image to the activations\n",
    "# # of the last conv layer\n",
    "# last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "# last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "# # Second, we create a model that maps the activations of the last conv\n",
    "# # layer to the final class predictions\n",
    "# classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "# x = classifier_input\n",
    "# for layer_name in classifier_layer_names:\n",
    "#     x = model.get_layer(layer_name)(x)\n",
    "# classifier_model = tf.keras.Model(classifier_input, x)\n",
    "\n",
    "# For GradCam++\n",
    "\n",
    "conv_layer = model.get_layer(\"relu\")\n",
    "heatmap_model = tf.keras.Model([model.inputs], [conv_layer.output, model.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = []\n",
    "preds = []\n",
    "img = []\n",
    "ids = []\n",
    "for idx, i in enumerate(test_dataset):\n",
    "    if (y_test[idx] == 1):\n",
    "        img_array = np.reshape(i, (1, 256, 256, 3))\n",
    "        pred = model.predict(img_array)\n",
    "        if(pred >= 0.9):\n",
    "            img.append(img_array)\n",
    "#             heat_map.append(show_heatmap(img_array, last_conv_layer_model, classifier_model))\n",
    "            heat_map.append(grad_cam_plus(heatmap_model, img_array))\n",
    "            preds.append(pred)\n",
    "            ids.append(idx)\n",
    "                        \n",
    "preds = np.array(preds).reshape(len(preds,))\n",
    "ids = np.array(ids).reshape(len(ids,))\n",
    "order = np.argsort(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in order[-20:]:\n",
    "    print(preds[i])\n",
    "    plt.imshow(np.reshape(heat_map[i], (256, 256, 3)))\n",
    "    fname = 'imgs/imgs_mimic/external/{a}_{b}.jpg'.format(a=ids[i], b=preds[i])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap = np.mean(heat_map, axis=0)\n",
    "plt.imshow(np.reshape(mean_heatmap, (256, 256, 3)))\n",
    "fname = 'imgs/imgs_mimic/external/mean.jpg'.format(a=ids[i], b=preds[i])\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saliency_map(model, image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = model(image)\n",
    "\n",
    "    # Get the gradients of the loss w.r.t to the input image.\n",
    "    gradient = tape.gradient(loss, image)\n",
    "\n",
    "    dgrad_abs = tf.math.abs(gradient)\n",
    "#         print(dgrad_abs.shape)\n",
    "    dgrad_max_ = np.max(dgrad_abs, axis=-1)[0]\n",
    "#         print(dgrad_max_.shape)\n",
    "\n",
    "    # normaliz between 0 and 1\n",
    "    arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "    smap = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
    "\n",
    "    return smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-1].output)\n",
    "for i in order[-20:]:\n",
    "    print(preds[i])\n",
    "    smap = get_saliency_map(linear_model, tf.Variable(img[i], dtype=float))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img[i][0])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(smap, cmap='Reds')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaps = []\n",
    "for i in range(len(img)):\n",
    "    smaps.append(get_saliency_map(linear_model, tf.Variable(img[i], dtype=float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np.mean(img, axis=0)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mean_img[0], cmap='gray')\n",
    "\n",
    "mean_smap = np.mean(smaps, axis=0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mean_smap, cmap='Reds')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
