{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from GradCAM import *\n",
    "from define_model import *\n",
    "from load_label import *\n",
    "from utilities import *\n",
    "from Parse_TFrecords import *\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "print(gpus)\n",
    " \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'checkpoints/checkpoints_mimic/checkpoint_BCE_Xception'\n",
    "\n",
    "\n",
    "model = define_model('Xception')\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "# For GradCam\n",
    "\n",
    "# First, we create a model that maps the input image to the activations\n",
    "# of the last conv layer\n",
    "last_conv_layer = model.get_layer(model.layers[-3].name)\n",
    "last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "# Second, we create a model that maps the activations of the last conv\n",
    "# layer to the final class predictions\n",
    "classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "x = classifier_input\n",
    "for layer_name in model.layers[-2:]:\n",
    "    x = model.get_layer(layer_name.name)(x)\n",
    "classifier_model = tf.keras.Model(classifier_input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = 'mimic'\n",
    "\n",
    "record_file_test = 'tfrecords/copd_{a}_test.tfrecords'.format(a=dataset)\n",
    "test_dataset = (tf.data.TFRecordDataset(\n",
    "    record_file_test, buffer_size=BATCH_SIZE, compression_type=None, num_parallel_reads=32)\n",
    ".map(parse_TFrecord_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = []\n",
    "heat_map_pp = []\n",
    "preds = []\n",
    "ids = []\n",
    "\n",
    "for idx, (i, l) in enumerate(test_dataset):\n",
    "    \n",
    "    if (l.numpy() != 1):\n",
    "        continue\n",
    "\n",
    "    img_array = np.reshape(i, (1, 256, 256, 3))\n",
    "    pred = model.predict(img_array)\n",
    "    \n",
    "    if (pred[0][0] >= 0.9):\n",
    "        heat_map.append(show_heatmap(img_array, last_conv_layer_model, classifier_model))\n",
    "        heat_map_pp.append(grad_cam_plus(img_array, last_conv_layer_model, classifier_model))\n",
    "        preds.append(pred)\n",
    "        ids.append(idx)\n",
    "    else:\n",
    "        continue\n",
    "                        \n",
    "preds = np.array(preds).reshape(len(preds,))\n",
    "ids = np.array(ids).reshape(len(ids,))\n",
    "order = np.argsort(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in order[-4:]:\n",
    "    print(preds[i])\n",
    "    plt.imshow(np.reshape(heat_map[i], (256, 256, 3)))\n",
    "    fname = 'imgs/mimic_test/{a}_{b}_gradcam.jpg'.format(a=ids[i], b=preds[i])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap = np.mean(heat_map, axis=0)\n",
    "plt.imshow(np.reshape(mean_heatmap, (256, 256, 3)))\n",
    "fname = 'imgs/mimic_test/mean_gradcam.jpg'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in order[-4:]:\n",
    "    print(preds[i])\n",
    "    plt.imshow(np.reshape(heat_map_pp[i], (256, 256, 3)))\n",
    "    fname = 'imgs/mimic_test/{a}_{b}_gradcampp.jpg'.format(a=ids[i], b=preds[i])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap = np.mean(heat_map_pp, axis=0)\n",
    "plt.imshow(np.reshape(mean_heatmap, (256, 256, 3)))\n",
    "fname = 'imgs/mimic_test/mean_gradcampp.jpg'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "dataset = 'emory'\n",
    "\n",
    "record_file_test = 'tfrecords/copd_{a}_test.tfrecords'.format(a=dataset)\n",
    "test_dataset = (tf.data.TFRecordDataset(\n",
    "    record_file_test, buffer_size=BATCH_SIZE, compression_type=None, num_parallel_reads=32)\n",
    ".map(parse_TFrecord_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = []\n",
    "heat_map_pp = []\n",
    "preds = []\n",
    "ids = []\n",
    "\n",
    "for idx, (i, l) in enumerate(test_dataset):\n",
    "    \n",
    "    if (l.numpy() != 1):\n",
    "        continue\n",
    "\n",
    "    img_array = np.reshape(i, (1, 256, 256, 3))\n",
    "    pred = model.predict(img_array)\n",
    "    \n",
    "    if (pred[0][0] >= 0.9):\n",
    "        heat_map.append(show_heatmap(img_array, last_conv_layer_model, classifier_model))\n",
    "        heat_map_pp.append(grad_cam_plus(img_array, last_conv_layer_model, classifier_model))\n",
    "        preds.append(pred)\n",
    "        ids.append(idx)\n",
    "    else:\n",
    "        continue\n",
    "                        \n",
    "preds = np.array(preds).reshape(len(preds,))\n",
    "ids = np.array(ids).reshape(len(ids,))\n",
    "order = np.argsort(np.array(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in order[-4:]:\n",
    "    print(preds[i])\n",
    "    plt.imshow(np.reshape(heat_map[i], (256, 256, 3)))\n",
    "    fname = 'imgs/emory_test/{a}_{b}_gradcam.jpg'.format(a=ids[i], b=preds[i])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap = np.mean(heat_map, axis=0)\n",
    "plt.imshow(np.reshape(mean_heatmap, (256, 256, 3)))\n",
    "fname = 'imgs/emory_test/mean_gradcam.jpg'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in order[-4:]:\n",
    "    print(preds[i])\n",
    "    plt.imshow(np.reshape(heat_map_pp[i], (256, 256, 3)))\n",
    "    fname = 'imgs/emory_test/{a}_{b}_gradcampp.jpg'.format(a=ids[i], b=preds[i])\n",
    "    plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_heatmap = np.mean(heat_map_pp, axis=0)\n",
    "plt.imshow(np.reshape(mean_heatmap, (256, 256, 3)))\n",
    "fname = 'imgs/emory_test/mean_gradcampp.jpg'\n",
    "plt.savefig(fname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mann-Whitney test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archis = ['DenseNet121', 'DenseNet201', 'ResNet50V2', 'ResNet152V2', 'Xception', 'InceptionV3', 'InceptionResNetV2', 'MobileNetV2']\n",
    "data = 'mimic'\n",
    "split = 'test'\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i in archis:\n",
    "    outfile = 'preds/{d}/{j}_preds/{i}_preds.npy'.format(d=data,j=split, i=i)\n",
    "\n",
    "    df[i] = np.reshape(np.load(outfile), (-1)).tolist()\n",
    "    \n",
    "y_label = get_data_label('mimic', split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0 = np.where(y_label==0)[0]\n",
    "label_1 = np.where(y_label==1)[0]\n",
    "\n",
    "df_0 = df.iloc[label_0]\n",
    "df_1 = df.iloc[label_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy import stats\n",
    "\n",
    "mat = [[0 for _ in range (len(archis))] for _ in range(len(archis))]\n",
    "\n",
    "for i in range(len(archis)):\n",
    "    for j in range(i+1, len(archis)):\n",
    "        u1, p = mannwhitneyu(df[archis[j]], df[archis[i]])\n",
    "        mat[j][i] = p\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# plt.matshow(np.array(mat))\n",
    "plt.title('Mannâ€“Whitney U test', fontsize = 20)\n",
    "sns.heatmap(np.array(mat), annot=True, xticklabels=archis, yticklabels=archis, cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = [[0 for _ in range (len(archis))] for _ in range(len(archis))]\n",
    "\n",
    "for i in range(len(archis)):\n",
    "    for j in range(i+1, len(archis)):\n",
    "        s, p = stats.ttest_ind(df[archis[j]], df[archis[i]], equal_var=False)\n",
    "        mat[j][i] = p\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# plt.matshow(np.array(mat))\n",
    "plt.title('T-test', fontsize = 20)\n",
    "sns.heatmap(np.array(mat), annot=True, xticklabels=archis, yticklabels=archis, cmap='magma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saliency Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saliency_map(model, image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(image)\n",
    "        loss = model(image)\n",
    "\n",
    "    # Get the gradients of the loss w.r.t to the input image.\n",
    "    gradient = tape.gradient(loss, image)\n",
    "\n",
    "    dgrad_abs = tf.math.abs(gradient)\n",
    "#         print(dgrad_abs.shape)\n",
    "    dgrad_max_ = np.max(dgrad_abs, axis=-1)[0]\n",
    "#         print(dgrad_max_.shape)\n",
    "\n",
    "    # normaliz between 0 and 1\n",
    "    arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)\n",
    "    smap = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)\n",
    "\n",
    "    return smap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Model(inputs=model.input, outputs=model.layers[-1].output)\n",
    "for i in order[-20:]:\n",
    "    print(preds[i])\n",
    "    smap = get_saliency_map(linear_model, tf.Variable(img[i], dtype=float))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img[i][0])\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(smap, cmap='Reds')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaps = []\n",
    "for i in range(len(img)):\n",
    "    smaps.append(get_saliency_map(linear_model, tf.Variable(img[i], dtype=float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_img = np.mean(img, axis=0)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mean_img[0], cmap='gray')\n",
    "\n",
    "mean_smap = np.mean(smaps, axis=0)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mean_smap, cmap='Reds')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
